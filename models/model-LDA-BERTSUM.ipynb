{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverzhou/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/oliverzhou/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverzhou/Library/Python/3.9/lib/python/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context=\"notebook\", \n",
    "              style=\"white\")\n",
    "\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from transformers import (BartTokenizer,\n",
    "                          BartForConditionalGeneration,\n",
    "                          DataCollatorForSeq2Seq,\n",
    "                          EarlyStoppingCallback,\n",
    "                          Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer, \n",
    "                          get_scheduler)\n",
    "import torch\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/oliverzhou/.cache/kagglehub/datasets/asaniczka/data-scientist-linkedin-job-postings/versions/103\n",
      "postings.csv\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/datasets/asaniczka/data-scientist-linkedin-job-postings\n",
    "import kagglehub\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"asaniczka/data-scientist-linkedin-job-postings\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "files = os.listdir(path)\n",
    "# Print the names of the files\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_link</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>job level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical Data Analyst</td>\n",
       "      <td>Jefferson Health Plans</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/technical-d...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Phoenixville</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Why Choose Jefferson Health Plans?\\nWe are an ...</td>\n",
       "      <td>KNIME, QlikView, SQL, MS Access, MS Excel, Log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Center Engineer - Minneapolis</td>\n",
       "      <td>DeRisk Technologies</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-center...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Job Responsibilities:\\nDeployment / In-Scope C...</td>\n",
       "      <td>Server, Storage, Backup, Networking, Virtualiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Avani Tech Solutions Private Limited</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Onsite</td>\n",
       "      <td>Success Factor knowledge\\nSchedule : Monday th...</td>\n",
       "      <td>Data Management, HR Data Retention Controls, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer II - NBC Sports Next</td>\n",
       "      <td>NBC Sports Next</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-engine...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Company Description\\nNBC Sports Next is where ...</td>\n",
       "      <td>Data Engineering, Data Warehousing, SQL, MySQL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Operational Assessment</td>\n",
       "      <td>National Grid Renewables</td>\n",
       "      <td>Bloomington, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>National Grid Renewables is a leading North Am...</td>\n",
       "      <td>Data Analyst, Operational Assessment, Wind Ene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title  \\\n",
       "0                 Technical Data Analyst   \n",
       "1     Data Center Engineer - Minneapolis   \n",
       "2                           Data Analyst   \n",
       "3     Data Engineer II - NBC Sports Next   \n",
       "4  Data Analyst - Operational Assessment   \n",
       "\n",
       "                                company      job_location  \\\n",
       "0                Jefferson Health Plans  Philadelphia, PA   \n",
       "1                   DeRisk Technologies   Minneapolis, MN   \n",
       "2  Avani Tech Solutions Private Limited   Minneapolis, MN   \n",
       "3                       NBC Sports Next   Minneapolis, MN   \n",
       "4              National Grid Renewables   Bloomington, MN   \n",
       "\n",
       "                                            job_link  first_seen  \\\n",
       "0  https://www.linkedin.com/jobs/view/technical-d...  2023-12-20   \n",
       "1  https://www.linkedin.com/jobs/view/data-center...  2023-12-20   \n",
       "2  https://www.linkedin.com/jobs/view/data-analys...  2023-12-20   \n",
       "3  https://www.linkedin.com/jobs/view/data-engine...  2023-12-20   \n",
       "4  https://www.linkedin.com/jobs/view/data-analys...  2023-12-20   \n",
       "\n",
       "    search_city search_country  job level job_type  \\\n",
       "0  Phoenixville  United States  Associate   Remote   \n",
       "1   Minneapolis  United States  Associate   Onsite   \n",
       "2   Minneapolis  United States  Associate   Onsite   \n",
       "3   Minneapolis  United States  Associate   Remote   \n",
       "4   Minneapolis  United States  Associate   Hybrid   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
       "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
       "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
       "3  Company Description\\nNBC Sports Next is where ...   \n",
       "4  National Grid Renewables is a leading North Am...   \n",
       "\n",
       "                                          job_skills  \n",
       "0  KNIME, QlikView, SQL, MS Access, MS Excel, Log...  \n",
       "1  Server, Storage, Backup, Networking, Virtualiz...  \n",
       "2  Data Management, HR Data Retention Controls, C...  \n",
       "3  Data Engineering, Data Warehousing, SQL, MySQL...  \n",
       "4  Data Analyst, Operational Assessment, Wind Ene...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings = pd.read_csv(path+'/postings.csv')\n",
    "postings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_link</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>job level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical Data Analyst</td>\n",
       "      <td>Jefferson Health Plans</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/technical-d...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Phoenixville</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>Why Choose Jefferson Health Plans?\\nWe are an ...</td>\n",
       "      <td>KNIME, QlikView, SQL, MS Access, MS Excel, Log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Center Engineer - Minneapolis</td>\n",
       "      <td>DeRisk Technologies</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-center...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Job Responsibilities:\\nDeployment / In-Scope C...</td>\n",
       "      <td>Server, Storage, Backup, Networking, Virtualiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Avani Tech Solutions Private Limited</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>Success Factor knowledge\\nSchedule : Monday th...</td>\n",
       "      <td>Data Management, HR Data Retention Controls, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer II - NBC Sports Next</td>\n",
       "      <td>NBC Sports Next</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-engine...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Company Description\\nNBC Sports Next is where ...</td>\n",
       "      <td>Data Engineering, Data Warehousing, SQL, MySQL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Operational Assessment</td>\n",
       "      <td>National Grid Renewables</td>\n",
       "      <td>Bloomington, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>National Grid Renewables is a leading North Am...</td>\n",
       "      <td>Data Analyst, Operational Assessment, Wind Ene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title  \\\n",
       "0                 Technical Data Analyst   \n",
       "1     Data Center Engineer - Minneapolis   \n",
       "2                           Data Analyst   \n",
       "3     Data Engineer II - NBC Sports Next   \n",
       "4  Data Analyst - Operational Assessment   \n",
       "\n",
       "                                company      job_location  \\\n",
       "0                Jefferson Health Plans  Philadelphia, PA   \n",
       "1                   DeRisk Technologies   Minneapolis, MN   \n",
       "2  Avani Tech Solutions Private Limited   Minneapolis, MN   \n",
       "3                       NBC Sports Next   Minneapolis, MN   \n",
       "4              National Grid Renewables   Bloomington, MN   \n",
       "\n",
       "                                            job_link  first_seen  \\\n",
       "0  https://www.linkedin.com/jobs/view/technical-d...  2023-12-20   \n",
       "1  https://www.linkedin.com/jobs/view/data-center...  2023-12-20   \n",
       "2  https://www.linkedin.com/jobs/view/data-analys...  2023-12-20   \n",
       "3  https://www.linkedin.com/jobs/view/data-engine...  2023-12-20   \n",
       "4  https://www.linkedin.com/jobs/view/data-analys...  2023-12-20   \n",
       "\n",
       "    search_city search_country  job level           job_type  \\\n",
       "0  Phoenixville  United States  Associate  Data Analyst (BI)   \n",
       "1   Minneapolis  United States  Associate      Data Engineer   \n",
       "2   Minneapolis  United States  Associate  Data Analyst (BI)   \n",
       "3   Minneapolis  United States  Associate      Data Engineer   \n",
       "4   Minneapolis  United States  Associate  Data Analyst (BI)   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
       "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
       "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
       "3  Company Description\\nNBC Sports Next is where ...   \n",
       "4  National Grid Renewables is a leading North Am...   \n",
       "\n",
       "                                          job_skills  \n",
       "0  KNIME, QlikView, SQL, MS Access, MS Excel, Log...  \n",
       "1  Server, Storage, Backup, Networking, Virtualiz...  \n",
       "2  Data Management, HR Data Retention Controls, C...  \n",
       "3  Data Engineering, Data Warehousing, SQL, MySQL...  \n",
       "4  Data Analyst, Operational Assessment, Wind Ene...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply conditions to create 'job_type'\n",
    "patterns = {\n",
    "    'Data Scientist': r'Data\\s*Scientist|Data\\s*Science|Scientist',  # Match both \"Data Scientist\" and \"Data Science\"\n",
    "    'Data Analyst (BI)': r'Data\\s*Analyst|Data\\s*Research\\s*Analyst|Analyst|Data\\s*Analytics|BI|Business\\s*Intelligence|Analytics|Visualization|Data\\s*Analysis',  # Add \"Data Research Analyst\"\n",
    "    'Data Engineer': r'Data\\s*Engineer|Database\\s*Engineer|Engineer',  # Consider \"Database Engineer\" as well\n",
    "    'Software Engineer': r'Software\\s*Engineer|Developer|Programmer|Software',\n",
    "    'Statistician': r'\\s*Statistician',\n",
    "    'Modeler': r'\\s*Modeler',\n",
    "    'Consultant': r'\\s*Consultant',\n",
    "    'Specialist': r'\\s*Specialist'\n",
    "}\n",
    "\n",
    "# Initialize 'job_type' column with 'Unknown'\n",
    "postings['job_type'] = 'Unknown'\n",
    "\n",
    "# Apply patterns to classify job titles\n",
    "for job_type, pattern in patterns.items():\n",
    "    postings.loc[postings['job_title'].str.contains(pattern, case=False, na=False, regex=True), 'job_type'] = job_type\n",
    "\n",
    "# Show the first few rows\n",
    "postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_link</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>search_city</th>\n",
       "      <th>search_country</th>\n",
       "      <th>job level</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical Data Analyst</td>\n",
       "      <td>Jefferson Health Plans</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/technical-d...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Phoenixville</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>Why Choose Jefferson Health Plans?\\nWe are an ...</td>\n",
       "      <td>KNIME, QlikView, SQL, MS Access, MS Excel, Log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Center Engineer - Minneapolis</td>\n",
       "      <td>DeRisk Technologies</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-center...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Job Responsibilities:\\nDeployment / In-Scope C...</td>\n",
       "      <td>Server, Storage, Backup, Networking, Virtualiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Avani Tech Solutions Private Limited</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>Success Factor knowledge\\nSchedule : Monday th...</td>\n",
       "      <td>Data Management, HR Data Retention Controls, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer II - NBC Sports Next</td>\n",
       "      <td>NBC Sports Next</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-engine...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Company Description\\nNBC Sports Next is where ...</td>\n",
       "      <td>Data Engineering, Data Warehousing, SQL, MySQL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Operational Assessment</td>\n",
       "      <td>National Grid Renewables</td>\n",
       "      <td>Bloomington, MN</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/data-analys...</td>\n",
       "      <td>2023-12-20</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>United States</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>National Grid Renewables is a leading North Am...</td>\n",
       "      <td>Data Analyst, Operational Assessment, Wind Ene...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title  \\\n",
       "0                 Technical Data Analyst   \n",
       "1     Data Center Engineer - Minneapolis   \n",
       "2                           Data Analyst   \n",
       "3     Data Engineer II - NBC Sports Next   \n",
       "4  Data Analyst - Operational Assessment   \n",
       "\n",
       "                                company      job_location  \\\n",
       "0                Jefferson Health Plans  Philadelphia, PA   \n",
       "1                   DeRisk Technologies   Minneapolis, MN   \n",
       "2  Avani Tech Solutions Private Limited   Minneapolis, MN   \n",
       "3                       NBC Sports Next   Minneapolis, MN   \n",
       "4              National Grid Renewables   Bloomington, MN   \n",
       "\n",
       "                                            job_link  first_seen  \\\n",
       "0  https://www.linkedin.com/jobs/view/technical-d...  2023-12-20   \n",
       "1  https://www.linkedin.com/jobs/view/data-center...  2023-12-20   \n",
       "2  https://www.linkedin.com/jobs/view/data-analys...  2023-12-20   \n",
       "3  https://www.linkedin.com/jobs/view/data-engine...  2023-12-20   \n",
       "4  https://www.linkedin.com/jobs/view/data-analys...  2023-12-20   \n",
       "\n",
       "    search_city search_country  job level           job_type  \\\n",
       "0  Phoenixville  United States  Associate  Data Analyst (BI)   \n",
       "1   Minneapolis  United States  Associate      Data Engineer   \n",
       "2   Minneapolis  United States  Associate  Data Analyst (BI)   \n",
       "3   Minneapolis  United States  Associate      Data Engineer   \n",
       "4   Minneapolis  United States  Associate  Data Analyst (BI)   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
       "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
       "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
       "3  Company Description\\nNBC Sports Next is where ...   \n",
       "4  National Grid Renewables is a leading North Am...   \n",
       "\n",
       "                                          job_skills  \n",
       "0  KNIME, QlikView, SQL, MS Access, MS Excel, Log...  \n",
       "1  Server, Storage, Backup, Networking, Virtualiz...  \n",
       "2  Data Management, HR Data Retention Controls, C...  \n",
       "3  Data Engineering, Data Warehousing, SQL, MySQL...  \n",
       "4  Data Analyst, Operational Assessment, Wind Ene...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[postings['job_type']=='Unknown']['job_title'].value_counts()\n",
    "data = postings[postings['job_type']!='Unknown']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "Cleaning Text: Remove unwanted characters, URLs, and unnecessary whitespace.\n",
    "Lowercasing: Convert all text to lowercase to maintain consistency.\n",
    "Tokenization: Split the text into words or tokens.\n",
    "Stop Words Removal: Remove common words that may not add value to your analysis (e.g., \"and\", \"the\").\n",
    "Stemming/Lemmatization: Reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical Data Analyst</td>\n",
       "      <td>Why Choose Jefferson Health Plans?\\nWe are an ...</td>\n",
       "      <td>KNIME, QlikView, SQL, MS Access, MS Excel, Log...</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Center Engineer - Minneapolis</td>\n",
       "      <td>Job Responsibilities:\\nDeployment / In-Scope C...</td>\n",
       "      <td>Server, Storage, Backup, Networking, Virtualiz...</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Success Factor knowledge\\nSchedule : Monday th...</td>\n",
       "      <td>Data Management, HR Data Retention Controls, C...</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer II - NBC Sports Next</td>\n",
       "      <td>Company Description\\nNBC Sports Next is where ...</td>\n",
       "      <td>Data Engineering, Data Warehousing, SQL, MySQL...</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Operational Assessment</td>\n",
       "      <td>National Grid Renewables is a leading North Am...</td>\n",
       "      <td>Data Analyst, Operational Assessment, Wind Ene...</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title  \\\n",
       "0                 Technical Data Analyst   \n",
       "1     Data Center Engineer - Minneapolis   \n",
       "2                           Data Analyst   \n",
       "3     Data Engineer II - NBC Sports Next   \n",
       "4  Data Analyst - Operational Assessment   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
       "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
       "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
       "3  Company Description\\nNBC Sports Next is where ...   \n",
       "4  National Grid Renewables is a leading North Am...   \n",
       "\n",
       "                                          job_skills           job_type  \n",
       "0  KNIME, QlikView, SQL, MS Access, MS Excel, Log...  Data Analyst (BI)  \n",
       "1  Server, Storage, Backup, Networking, Virtualiz...      Data Engineer  \n",
       "2  Data Management, HR Data Retention Controls, C...  Data Analyst (BI)  \n",
       "3  Data Engineering, Data Warehousing, SQL, MySQL...      Data Engineer  \n",
       "4  Data Analyst, Operational Assessment, Wind Ene...  Data Analyst (BI)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['job_title', 'job_summary', 'job_skills', 'job_type']\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_type\n",
       "Data Scientist       1691\n",
       "Data Engineer        1335\n",
       "Data Analyst (BI)     895\n",
       "Software Engineer     282\n",
       "Specialist             49\n",
       "Consultant             44\n",
       "Statistician           11\n",
       "Modeler                 8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['job_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title      0\n",
       "job_summary    0\n",
       "job_skills     0\n",
       "job_type       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(subset=['job_title', 'job_summary', 'job_skills'], inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oliverzhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/oliverzhou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oliverzhou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import json\n",
    "\n",
    "# Load JSON file\n",
    "with open(\"skills.json\", \"r\") as file:\n",
    "    skills_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_type</th>\n",
       "      <th>cleaned_job_summary</th>\n",
       "      <th>cleaned_job_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technical Data Analyst</td>\n",
       "      <td>Why Choose Jefferson Health Plans?\\nWe are an ...</td>\n",
       "      <td>KNIME, QlikView, SQL, MS Access, MS Excel, Log...</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>choose jefferson health plan awardwinning notf...</td>\n",
       "      <td>knime qlikview sql ms access ms excel logical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Center Engineer - Minneapolis</td>\n",
       "      <td>Job Responsibilities:\\nDeployment / In-Scope C...</td>\n",
       "      <td>Server, Storage, Backup, Networking, Virtualiz...</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>job responsibility deployment inscope configur...</td>\n",
       "      <td>server storage backup networking virtualizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Success Factor knowledge\\nSchedule : Monday th...</td>\n",
       "      <td>Data Management, HR Data Retention Controls, C...</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>success factor knowledge schedule monday frida...</td>\n",
       "      <td>data management hr data retention controls cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer II - NBC Sports Next</td>\n",
       "      <td>Company Description\\nNBC Sports Next is where ...</td>\n",
       "      <td>Data Engineering, Data Warehousing, SQL, MySQL...</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>company description nbc sport next sport techn...</td>\n",
       "      <td>data engineering data warehousing sql mysql po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Operational Assessment</td>\n",
       "      <td>National Grid Renewables is a leading North Am...</td>\n",
       "      <td>Data Analyst, Operational Assessment, Wind Ene...</td>\n",
       "      <td>Data Analyst (BI)</td>\n",
       "      <td>national grid renewables leading north america...</td>\n",
       "      <td>data analyst operational assessment wind energ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title  \\\n",
       "0                 Technical Data Analyst   \n",
       "1     Data Center Engineer - Minneapolis   \n",
       "2                           Data Analyst   \n",
       "3     Data Engineer II - NBC Sports Next   \n",
       "4  Data Analyst - Operational Assessment   \n",
       "\n",
       "                                         job_summary  \\\n",
       "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
       "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
       "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
       "3  Company Description\\nNBC Sports Next is where ...   \n",
       "4  National Grid Renewables is a leading North Am...   \n",
       "\n",
       "                                          job_skills           job_type  \\\n",
       "0  KNIME, QlikView, SQL, MS Access, MS Excel, Log...  Data Analyst (BI)   \n",
       "1  Server, Storage, Backup, Networking, Virtualiz...      Data Engineer   \n",
       "2  Data Management, HR Data Retention Controls, C...  Data Analyst (BI)   \n",
       "3  Data Engineering, Data Warehousing, SQL, MySQL...      Data Engineer   \n",
       "4  Data Analyst, Operational Assessment, Wind Ene...  Data Analyst (BI)   \n",
       "\n",
       "                                 cleaned_job_summary  \\\n",
       "0  choose jefferson health plan awardwinning notf...   \n",
       "1  job responsibility deployment inscope configur...   \n",
       "2  success factor knowledge schedule monday frida...   \n",
       "3  company description nbc sport next sport techn...   \n",
       "4  national grid renewables leading north america...   \n",
       "\n",
       "                                  cleaned_job_skills  \n",
       "0  knime qlikview sql ms access ms excel logical ...  \n",
       "1  server storage backup networking virtualizatio...  \n",
       "2  data management hr data retention controls cal...  \n",
       "3  data engineering data warehousing sql mysql po...  \n",
       "4  data analyst operational assessment wind energ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text cleaning for job_summary\n",
    "def clean_text_summary(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Retain keywords (skills) and remove stop words\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(word) for word in tokens \n",
    "        if word in skills_data or word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "# Text cleaning for job_skills (only remove special characters and notations)\n",
    "def clean_text_skills(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Assuming your DataFrame is named 'data'\n",
    "# Apply cleaning to job_summary and job_skills\n",
    "data['cleaned_job_summary'] = data['job_summary'].apply(clean_text_summary)\n",
    "data['cleaned_job_skills'] = data['job_skills'].apply(clean_text_skills)\n",
    "\n",
    "# Save the cleaned data\n",
    "data.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "# Preview the cleaned data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTSUM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.bert.modeling_bert.BertModel'> is overwritten by shared encoder config: BertConfig {\n",
      "  \"_name_or_path\": \"google/bert_uncased_L-4_H-512_A-8\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"return_dict\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.bert.modeling_bert.BertLMHeadModel'> is overwritten by shared decoder config: BertConfig {\n",
      "  \"_name_or_path\": \"google/bert_uncased_L-4_H-512_A-8\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"return_dict\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 0/431...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverzhou/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (30). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 100/431...\n",
      "Processing text 200/431...\n",
      "Processing text 300/431...\n",
      "Processing text 400/431...\n",
      "ROUGE Scores with BERTSUM: {'rouge1': np.float64(6.0), 'rouge2': np.float64(2.2), 'rougeL': np.float64(5.9), 'rougeLsum': np.float64(5.9)}\n",
      "Original Text: role data analyst location hartford ct raleigh nc duration fulltime job description least year experience working healthcare business data analyst health plan member enrollment benefit plan configuration provider setup contract setup billing payment ee claim processing edi transaction medicare medicaid commercial health plan cob accumulator least year experience requirement elicitation technique like jadsessions workshop interview survey etc hedis knowledgeable data analyst least year experience creating technical requirement specification based architecturedesign detailing processe least year experience agile methodology preferably agile scrum business analyst understanding u healthcare data thanks regard sheebakavipriya process specialist talent acquisition north america infy hr e sheebakavipriyapinfosyscom w wwwinfosyscom show show less\n",
      "Reference Summary: Data Analyst (BI)\n",
      "Generated Summary (BERTSUM): job description least year experience working healthcare business data analyst at the job description. sheebakavipriya process specialist talent acquisition north america infy\n",
      "--------------------------------------------------\n",
      "Original Text: recruiting scratch recruiting scratch premier talent firm focus placing best product manager software hardware talent innovative company team remote work team across united state help hire work company funded best investor including sequoia capital lightspeed venture tiger global management az accel dfj httpswwwrecruitingfromscratchcom hybrid role based palo alto san francisco chicago office require office tuesday thursday whats interesting role believe ai revolutionize dating industry data engineer lead responsible building high quality ml datasets scale used train ml model power aicentric feature pivotal role opportunity build foundational tool data pipeline ingest normalize clean valuable data would fundamental ml engineer build ai tool including recommendation llm ad visual search growthnotifications trust safety whats job looking exceptional data engineer passionate data ai value bring company love working data ops scale committed hard work necessary continuously improve ml data pipeline position responsible establishing executing strategy organization ml data engine initial focus agile ml data ops includes identification infrastructure component data stack used design implementation pipeline data system team automation workflow data enrichment monitoring tool ai model tech lead specialized data engineering expected code contribute stack responsibility dive dataset design implement scale data prepost processing pipeline ml model work applied ml solution area data mining cleaning normalizing modeling selfmotivated seeking solution correct path isnt always known collaborate engineer conceptualizing planning implementing data engineering initiative working different stakeholder design build data platform framework processing high volume data real time well batch used across engineering team build data processing stream cleaning modeling text data llm research evaluate new technology big data space guide continuous improvement collaborate multifunctional team help tune performance large data application work privacy security team data governance risk compliance initiative work initiative ensure stability performance reliability data infrastructure well love bachelor computer science mathematics physic related field year experience data engineer building productionlevel prepostprocessing data pipeline mldl model including year technical leadership experience experience statistical analysis visualization datasets using panda r experience designing building highly available distributed system data extraction ingestion normalization processing large data set real time well batch used across engineering team using orchestration framework like airflow kubeflow pipeline tool demonstrated prior experience creating data pipeline text data set nlp large language model ability produce wellengineered software including appropriate automated test suite technical documentation operational strategy excellent coding skill python java bash sql expertise git version control experience using big data technology snowflake airflow kubernetes docker helm spark pyspark experience public cloud environment aws gcp azure significant experience relational database query authoring sql well nosql database like dynamodb etc experience building maintaining etl managing highquality reliable etl pipeline well really swoon year experience technical leadership building data engineering pipeline ai previous experience building data pipeline conversational ai apis recommender system experience distributed system microservices experience kubernetes building docker image experience building streamprocessing system using solution kafka storm sparkstreaming strong understanding applied machine learning topic familiar legal compliance data management tool data classification retention consistent track record managing implementing complex data project youll love u mission impact worldleading lgbtq social networking service role impact life million lgbtq people around world multiple location hiring someone role based ideally san francisco palo alto family insurance insurance premium coverage health dental vision partial coverage dependent retirement saving generous k plan match immediate vest u compensation industrycompetitive compensation eligibility company bonus equity program queerinclusive benefit industryleading genderaffirming offering cost coverage access included health monthly stipend hrt additional benefit flexible vacation policy monthly stipend cell phone internet wellness food onetime homeoffice setup stipend companysponsored event base pay range usd httpswwwrecruitingfromscratchcom show show less\n",
      "Reference Summary: Data Engineer\n",
      "Generated Summary (BERTSUM): recruiting scratch premier talent firm focus placing best product manager software hardware hardware hardware talent innovative company team remote work team across united state help hire work company funded\n",
      "--------------------------------------------------\n",
      "Original Text: aurora co icr opportunity available experienced motivated data scientist bayesian statistic background essential job responsibility data scientist expected work independently complex technical task require indepth data analysis methodology datainformation collection develop unique technical solution application intelligence field participate member multidisciplinary team analyze satisfy customer requirement develop implement validate document present finding specialized analysis software tool model required effectively handle concurrent technical task conflicting priority approach difficult problem enthusiasm creativity ability change focus necessary travel required skill qualification active top secret security clearance ssbi experience bayesian statistic time series analysis master degree statistic minimum two year professional experience phd statistic related field experience computational analysis tool developing software least one following programing language python preferred r c matlab experience developing applying advanced statisticalmachine learning model algorithm one following classification clustering anomaly detection density estimation data mining pattern recognition knowledge discovery experience extracting processing structuredunstructured data various source eg database text file web preparing data analysis collaborate others multidisciplinary team environment accomplish research goal ability work effectively smallteam setting solve complex problem ability desire obtain substantial domain knowledge field application ability communicate effectively subject matter expert proficient verbal written communication skill collaborate effectively team environment present explain technical information customer desired qualification experience intelligence community broad knowledge statistical analysis ml method problemsolving skill experience spatial statistic multivariate statistic gaussian process random forest support vector machine natural language processing neural network proficient python jupyter markdown experience working database sql andor elk stack experience creating data visualization dashboard position offer comprehensive benefit package includes company equity retirement plan companypaid health care benefit flexible paid time policy opportunity raise bonus year icr inc considers several factor extending job offer including limited candidate key skill relevant work experience education training certification show show less\n",
      "Reference Summary: Statistician\n",
      "Generated Summary (BERTSUM): aurora co icr opportunity available experienced motivated data scientist bayesian statistic background essential job responsibility data scientist expected work independently complex technical task require indept\n",
      "--------------------------------------------------\n",
      "Original Text: flexible work arrangement hybrid month program participant experience technically challenging handson six month rotation three core function pjm system operation system planning market well shorter rotation many support function including compliance information technology member service risk security program designed build fundamental understanding operation power grid realtime system operation perspective thorough knowledge wholesale market operate critical knowledge longterm infrastructure planning process position start june essential function corporate rotation program participant receive handson engineering experience support largest rtoiso participant receive exposure support senior executive leader assignment roundtable discussion participate formal mentoring partnership receive ongoing learning opportunity well interface program manager monitor progress development required characteristic qualification b degree engineering electrical engineering electrical power system engineering data science computer science economics mathematics information technology cyber security data science year experience gpa higher scale ability produce highquality work product attention detail experience quantitative qualitative analysis ability use mathematical electrical theory ability troubleshoot provide technical support show show less\n",
      "Reference Summary: Data Engineer\n",
      "Generated Summary (BERTSUM): flexible work arrangement hybrid month program participant experience technically challenging handson six month rotation three core function pjm system planning market well shorter rotation many support function\n",
      "--------------------------------------------------\n",
      "Original Text: data analyst leatherhead uk salary data analyst want use skill make real impact education world enjoy seeing work bringing people data together best way person want move forward greenfield space could role client really making life teacher child better within education know important best support possible time mean youll opportunity part something truly amazing helping support life thousand people program ensure key guidance structure always provided involved forever learning adapting create best future role see thick key asset bring data together work youll part help understand define strategy vision going forward allowing work greenfield environment data support youll glue really help create future role truly proud looking experience utilising analysing large data set ability clean define show data database design modelling experience working crm environment ability confidence liaise multiple department stakeholder sale andor marketing experience would desirable someone passionate great thing data youre looking part something amazing make real difference hit apply button show show less\n",
      "Reference Summary: Data Analyst (BI)\n",
      "Generated Summary (BERTSUM): data analyst leatherhead uk salary data analyst want use skill make real impact education world. youll opportunity part something truly amazing helping support life thousand people\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a BERT-based encoder-decoder model for summarization\n",
    "model_name = \"mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to summarize text using the encoder-decoder model\n",
    "def bertsum_summarize(text, max_length=30):\n",
    "    try:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        # Decode the generated summary\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error during BERTSUM summarization: {e}\")\n",
    "        return \"Error during summarization\"\n",
    "\n",
    "# Generate summaries for test data\n",
    "candidate_summaries = []\n",
    "for i, text in enumerate(texts):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing text {i}/{len(texts)}...\")\n",
    "    candidate_summaries.append(bertsum_summarize(text))\n",
    "\n",
    "# Save candidate summaries\n",
    "with open(\"bertsum-summaries.txt\", \"w\") as file:\n",
    "    for summary in candidate_summaries:\n",
    "        file.write(summary + \"\\n\")\n",
    "\n",
    "# Evaluate using ROUGE\n",
    "rouge_scores = calc_rouge_scores(candidate_summaries, ref_summaries)\n",
    "print(\"ROUGE Scores with BERTSUM:\", rouge_scores)\n",
    "\n",
    "# Output a few results for inspection\n",
    "for i in range(5):\n",
    "    print(f\"Original Text: {texts[i]}\")\n",
    "    print(f\"Reference Summary: {ref_summaries[i]}\")\n",
    "    print(f\"Generated Summary (BERTSUM): {candidate_summaries[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oliverzhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/oliverzhou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores for LDA Summarization: {'rouge1': np.float64(1.7), 'rouge2': np.float64(0.0), 'rougeL': np.float64(1.4), 'rougeLsum': np.float64(1.7)}\n",
      "                               job_title  \\\n",
      "0                 Technical Data Analyst   \n",
      "1     Data Center Engineer - Minneapolis   \n",
      "2                           Data Analyst   \n",
      "3     Data Engineer II - NBC Sports Next   \n",
      "4  Data Analyst - Operational Assessment   \n",
      "\n",
      "                                         job_summary  \\\n",
      "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
      "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
      "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
      "3  Company Description\\nNBC Sports Next is where ...   \n",
      "4  National Grid Renewables is a leading North Am...   \n",
      "\n",
      "                              lda_summary  \n",
      "0     analysis science work business team  \n",
      "1         technology team year skill work  \n",
      "2         technology team year skill work  \n",
      "3     analysis science work business team  \n",
      "4  business opportunity product work team  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from evaluate import load\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['tokenized_summary'] = data['cleaned_job_summary'].apply(\n",
    "    lambda x: [word for word in word_tokenize(x) if word not in stop_words]\n",
    ")\n",
    "\n",
    "# Convert tokenized summaries back to strings\n",
    "data['processed_summary'] = data['tokenized_summary'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(data['processed_summary'])\n",
    "\n",
    "# Fit LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "# Extract topics for each document\n",
    "def get_topics_per_doc(lda_model, doc_term_matrix, feature_names, num_words=5):\n",
    "    \"\"\"\n",
    "    Get the top words representing each topic in each document.\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    for topic_weights in lda_model.transform(doc_term_matrix):\n",
    "        topic = topic_weights.argmax()\n",
    "        top_words = [feature_names[i] for i in lda_model.components_[topic].argsort()[-num_words:]]\n",
    "        topics.append(' '.join(top_words))\n",
    "    return topics\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "data['lda_summary'] = get_topics_per_doc(lda_model, doc_term_matrix, feature_names)\n",
    "\n",
    "# Evaluate using ROUGE\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "def calc_rouge_scores(candidates, references):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores for candidate summaries against reference summaries.\n",
    "    \"\"\"\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    return {key: round(value * 100, 1) for key, value in result.items()}\n",
    "\n",
    "# Compute ROUGE scores\n",
    "lda_candidates = data['lda_summary'].tolist()\n",
    "reference_summaries = data['job_summary'].tolist()\n",
    "\n",
    "rouge_scores = calc_rouge_scores(lda_candidates, reference_summaries)\n",
    "print(\"ROUGE Scores for LDA Summarization:\", rouge_scores)\n",
    "\n",
    "# Display the first few results\n",
    "print(data[['job_title', 'job_summary', 'lda_summary']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach combines extractive summarization using TF-IDF cosine similarity and abstractive summarization using a pre-trained T5 transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/oliverzhou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores for Improved Summarization: {'rouge1': np.float64(15.9), 'rouge2': np.float64(9.0), 'rougeL': np.float64(15.8), 'rougeLsum': np.float64(15.8)}\n",
      "                               job_title  \\\n",
      "0                 Technical Data Analyst   \n",
      "1     Data Center Engineer - Minneapolis   \n",
      "2                           Data Analyst   \n",
      "3     Data Engineer II - NBC Sports Next   \n",
      "4  Data Analyst - Operational Assessment   \n",
      "\n",
      "                                         job_summary  \\\n",
      "0  Why Choose Jefferson Health Plans?\\nWe are an ...   \n",
      "1  Job Responsibilities:\\nDeployment / In-Scope C...   \n",
      "2  Success Factor knowledge\\nSchedule : Monday th...   \n",
      "3  Company Description\\nNBC Sports Next is where ...   \n",
      "4  National Grid Renewables is a leading North Am...   \n",
      "\n",
      "                                   generated_summary  \n",
      "0  jefferson health plan awardwinning notforprofi...  \n",
      "1  job responsibility deployment inscope configur...  \n",
      "2  success factor knowledge schedule monday frida...  \n",
      "3  nbc sport next sport technology intersect subd...  \n",
      "4  national grid renewables develops project corp...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "data['cleaned_job_summary'] = data['cleaned_job_summary'].fillna(\"\")\n",
    "\n",
    "# Step 1: Extractive Summarization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(data['cleaned_job_summary'])\n",
    "\n",
    "def extract_key_sentences(text, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Extract top sentences based on TF-IDF cosine similarity scores.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)  # Return all sentences if fewer than the threshold\n",
    "    \n",
    "    # Calculate sentence vectors\n",
    "    sentence_vectors = vectorizer.transform(sentences)\n",
    "    similarity_scores = cosine_similarity(sentence_vectors, tfidf_matrix)\n",
    "    \n",
    "    # Rank sentences by their average similarity score\n",
    "    ranked_indices = similarity_scores.mean(axis=1).argsort()[::-1][:num_sentences]\n",
    "    return ' '.join([sentences[i] for i in ranked_indices])\n",
    "\n",
    "# Extract key sentences\n",
    "data['key_sentences'] = data['cleaned_job_summary'].apply(extract_key_sentences)\n",
    "\n",
    "# Step 2: Abstractive Summarization using T5\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_summary(text, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate abstractive summaries using T5.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(inputs.input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return \"Error during summarization\"\n",
    "\n",
    "# Apply abstractive summarization on key sentences\n",
    "data['generated_summary'] = data['key_sentences'].apply(generate_summary)\n",
    "\n",
    "# Evaluate using ROUGE\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "def calc_rouge_scores(candidates, references):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores for candidate summaries against reference summaries.\n",
    "    \"\"\"\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    return {key: round(value * 100, 1) for key, value in result.items()}\n",
    "\n",
    "# Compute ROUGE scores\n",
    "candidates = data['generated_summary'].tolist()\n",
    "references = data['job_summary'].tolist()\n",
    "rouge_scores = calc_rouge_scores(candidates, references)\n",
    "print(\"ROUGE Scores for Improved Summarization:\", rouge_scores)\n",
    "\n",
    "# Display results\n",
    "print(data[['job_title', 'job_summary', 'generated_summary']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths of the Approach:\n",
    "\n",
    "The hybrid extractive-abstractive method outperforms simpler LDA-based summarization (e.g., previous ROUGE-1 ~1.7).\n",
    "Combining relevance filtering (TF-IDF) and abstractive refinement (T5) provides a more aligned and coherent summary.\n",
    "\n",
    "T5 abstractive summarization improves fluency, making the summaries more human-like compared to pure extractive methods.\n",
    "\n",
    "ROUGE-L scores suggest the generated summaries capture the structure and phrasing of the reference summaries to a reasonable degree.\n",
    "\n",
    "-------------\n",
    "\n",
    "Limitations:\n",
    "\n",
    "A ROUGE-2 score of 9.0 suggests the method struggles to consistently capture semantic pairings and context.\n",
    "The extractive step might omit key relational phrases that the abstractive model doesn't reconstruct.\n",
    "\n",
    "If the reference summaries are verbose or not concise, this can lower the scores.\n",
    "ROUGE might not fully capture semantic equivalence or rephrased content.\n",
    "\n",
    "------------\n",
    "The hybrid extractive-abstractive summarization approach shows significant improvement with ROUGE-1 and ROUGE-L nearing 16%. While there's room for improvement, the current results demonstrate a good balance between relevance and readability. Further refinement of extractive techniques and model tuning can yield even better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
